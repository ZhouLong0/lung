{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import *\n",
    "from PIL import Image\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from src.utils.utils import training, testing, EarlyStopping, get_y_true_preds\n",
    "from src.utils.LungDataset import LungSet\n",
    "from src.model.LungNetwork import LungNet\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "seed=2024\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "train_path = \"data/dataframes/df_train_7C_augmented.csv\"\n",
    "test_path = \"data/dataframes/df_test_7C_augmented.csv\"\n",
    "data_path = \"data/augmented_data\"\n",
    "oversampling = True\n",
    "\n",
    "baseline_path = \"src/model/tenpercent_resnet18.ckpt\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain= pd.read_csv(train_path).sample(frac = 1)\n",
    "dtest = pd.read_csv(test_path).sample(frac = 1)\n",
    "\n",
    "X_train = dtrain\n",
    "y_train = dtrain['label']\n",
    "\n",
    "X_ros, y_ros = RandomOverSampler(random_state=seed).fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>lame</th>\n",
       "      <th>patch</th>\n",
       "      <th>classe</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "      <th>TetraClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46764</th>\n",
       "      <td>6</td>\n",
       "      <td>L</td>\n",
       "      <td>L_6_Z (10)_HF.jpg</td>\n",
       "      <td>Lépidique</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>TuGr12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No_3_C(442).jpg</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61028</th>\n",
       "      <td>230</td>\n",
       "      <td>Gc</td>\n",
       "      <td>Gc_230_A (283).jpg</td>\n",
       "      <td>Glandulaire complexe</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>TuGr3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66207</th>\n",
       "      <td>231</td>\n",
       "      <td>S</td>\n",
       "      <td>S_231_A (380)_HF.jpg</td>\n",
       "      <td>Solide</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>TuGr3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27189</th>\n",
       "      <td>9</td>\n",
       "      <td>N</td>\n",
       "      <td>N_9_S (1116).tif</td>\n",
       "      <td>Nécrose</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Né</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47643</th>\n",
       "      <td>7</td>\n",
       "      <td>A</td>\n",
       "      <td>A_7_A (181)_HF.jpg</td>\n",
       "      <td>Acinaire</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>TuGr12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35456</th>\n",
       "      <td>7</td>\n",
       "      <td>F</td>\n",
       "      <td>F_7_A (353)_rot270.jpg</td>\n",
       "      <td>Fibrose</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51808</th>\n",
       "      <td>7</td>\n",
       "      <td>A</td>\n",
       "      <td>A_7_A (143).jpg</td>\n",
       "      <td>Acinaire</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>TuGr12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52730</th>\n",
       "      <td>13</td>\n",
       "      <td>D</td>\n",
       "      <td>13_D_row_97_col_29.jpg</td>\n",
       "      <td>Acinaire</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>TuGr12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7816</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No_3_C(73).jpg</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66649 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Patient lame                   patch                classe  label   \n",
       "46764        6    L       L_6_Z (10)_HF.jpg             Lépidique      5  \\\n",
       "5394         3   No         No_3_C(442).jpg                Normal      0   \n",
       "61028      230   Gc      Gc_230_A (283).jpg  Glandulaire complexe      6   \n",
       "66207      231    S    S_231_A (380)_HF.jpg                Solide      6   \n",
       "27189        9    N        N_9_S (1116).tif               Nécrose      2   \n",
       "...        ...  ...                     ...                   ...    ...   \n",
       "47643        7    A      A_7_A (181)_HF.jpg              Acinaire      5   \n",
       "35456        7    F  F_7_A (353)_rot270.jpg               Fibrose      4   \n",
       "51808        7    A         A_7_A (143).jpg              Acinaire      5   \n",
       "52730       13    D  13_D_row_97_col_29.jpg              Acinaire      5   \n",
       "7816         3   No          No_3_C(73).jpg                Normal      0   \n",
       "\n",
       "      dataset TetraClass  \n",
       "46764   train     TuGr12  \n",
       "5394    train          P  \n",
       "61028   train      TuGr3  \n",
       "66207   train      TuGr3  \n",
       "27189   train         Né  \n",
       "...       ...        ...  \n",
       "47643   train     TuGr12  \n",
       "35456   train         Fi  \n",
       "51808   train     TuGr12  \n",
       "52730   train     TuGr12  \n",
       "7816    train          P  \n",
       "\n",
       "[66649 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TetraClass\n",
       " P         17634\n",
       " Fi        12390\n",
       " TuGr3     10341\n",
       " TuGr12     9964\n",
       " Né         8994\n",
       " H          3960\n",
       " TL         3366\n",
       " Name: count, dtype: int64,\n",
       " TetraClass\n",
       " TuGr12    17634\n",
       " P         17634\n",
       " TuGr3     17634\n",
       " Né        17634\n",
       " Fi        17634\n",
       " H         17634\n",
       " TL        17634\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['TetraClass'].value_counts(), X_ros['TetraClass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if oversampling:\n",
    "    X_train, y_train = X_ros, y_ros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = tt.Compose([tt.RandomHorizontalFlip(), \n",
    "                    tt.RandomVerticalFlip(),\n",
    "                    tt.ToTensor()])\n",
    "t_test = tt.Compose([tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a LungSet object(torch.Datatset) with:\n",
    "# df(DataFrame): image_path, label\n",
    "# data_path(str): directory containing the images\n",
    "# transform(transforms.Compose): transformations to apply\n",
    "# __iter__: return Image, label\n",
    "train_set = LungSet(X_train, data_path=data_path)\n",
    "test_set = LungSet(dtest, data_path=data_path)\n",
    "\n",
    "# Create the torch.Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "model = LungNet(baseline_path, num_classes)\n",
    "# data paralellism in traning \n",
    "# model=nn.DataParallel(model, device_ids=[0, 1])\n",
    "# model=model.cuda()\n",
    "# print(\"trainable parameters\",sum([p.numel() for p in model.parameters() if p.requires_grad]))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.125)\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.001)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.1, weight_decay=0.01)\n",
    "# allow dynamic lr reducing based on some measurement (a metric has stopped improving)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.1, patience=10, min_lr=1e-15, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check avilable gpus\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/965 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/augmented_data/Solide/S_231_A (302).jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()               \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train one epoch for each batch                                                                                                         \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m epoch_train_loss, epoch_train_accuracy  \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m epoch_test_loss, epoch_test_accuracy    \u001b[38;5;241m=\u001b[39m testing(model, test_loader, criterion, device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# save the model if is currently the best\u001b[39;00m\n",
      "File \u001b[0;32m/media/eliott/inner_disk/lung/src/utils/utils.py:16\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):  \n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[1;32m     18\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/media/eliott/inner_disk/lung/src/utils/LungDataset.py:23\u001b[0m, in \u001b[0;36mLungSet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     22\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[idx]               \n\u001b[0;32m---> 23\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     25\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3236\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3237\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/augmented_data/Solide/S_231_A (302).jpg'"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "num_epoch=100\n",
    "comment=\" \"\n",
    "T=0\n",
    "\n",
    "train_loss, test_loss, train_accuracy, test_accuracy = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    t0 = time.time()               \n",
    "    # train one epoch for each batch                                                                                                         \n",
    "    epoch_train_loss, epoch_train_accuracy  = training(model, train_loader, criterion, optimizer, device)\n",
    "    epoch_test_loss, epoch_test_accuracy    = testing(model, test_loader, criterion, device)\n",
    "\n",
    "    # save the model if is currently the best\n",
    "    if best_acc < epoch_test_accuracy:\n",
    "        best_acc = epoch_test_accuracy\n",
    "        torch.save(model.state_dict(),     f\"model_checkpoints/best_model_{num_classes}C.pth\")\n",
    "        torch.save(optimizer.state_dict(), f\"model_checkpoints/best_optimizer_{num_classes}C.pth\")\n",
    "        comment=\" (loss decreased!) new best model saved\"\n",
    "    \n",
    "    # reduce the learning rate if test loss didn't decrease\n",
    "    scheduler.step(epoch_test_loss)\n",
    "\n",
    "    # keep track of the loss and accuracy\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    train_accuracy.append(epoch_train_accuracy)\n",
    "    test_loss.append(epoch_test_loss)\n",
    "    test_accuracy.append(epoch_test_accuracy)\n",
    "\n",
    "    # every 5 epoch save the loss and accuracy history. the model and the optimizer (lr changes)\n",
    "    if epoch!=0 and epoch%5 == 0:\n",
    "        np.save(f'evaluation/{num_classes}c_train_test_loss.npy', [train_loss, test_loss])\n",
    "        np.save(f'evaluation/{num_classes}c_train_test_accuracies.npy', [train_accuracy, test_accuracy])\n",
    "        torch.save(model.state_dict(),     f\"model_checkpoints/model_{num_classes}C.pth\")\n",
    "        torch.save(optimizer.state_dict(), f\"model_checkpoints/optimizer_{num_classes}C.pth\")\n",
    "        \n",
    "    print('Epoch {:g}/{:g}: TRAIN Loss={:0.5f} -- Acc={:0.3f}% '.format(epoch+1,num_epoch, epoch_train_loss, epoch_train_accuracy), end='')\n",
    "    print('|| TEST Loss={:0.5f} -- Acc={:0.3f}%  --- Time={:g}min'.format(epoch_test_loss,epoch_test_accuracy, (time.time()-t0)//60), end='')\n",
    "    print(comment)\n",
    "    comment=\" \"\n",
    "    T+=(time.time()-t0)\n",
    "\n",
    "    # early_stopping(epoch_test_loss)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(\"Early stopping.\")\n",
    "    #     break\n",
    "print(\"total time : T={:.3f}h\".format(T//3600))\n",
    "\n",
    "# save the final model\n",
    "np.save(f'evaluation/{num_classes}c_train_test_loss.npy', [train_loss, test_loss])\n",
    "np.save(f'evaluation/{num_classes}c_train_test_accuracies.npy', [train_accuracy, test_accuracy])\n",
    "torch.save(model.state_dict(),     f\"model_checkpoints/model_{num_classes}C.pth\")\n",
    "torch.save(optimizer.state_dict(), f\"model_checkpoints/optimizer_{num_classes}C.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
